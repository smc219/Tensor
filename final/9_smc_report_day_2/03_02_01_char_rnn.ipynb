{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_02_01_char_rnn (1).ipynb","provenance":[{"file_id":"1SIO9XzwKMquyDxRkwLS2qkZUE-3NiNuL","timestamp":1575620421258},{"file_id":"1R1h6CiVZbGVvKGAEYxnaf4o40ArUF99Q","timestamp":1575615157400},{"file_id":"1nkqVytRmAxDvYJwgoueea8kPBiCXirXw","timestamp":1574791737676},{"file_id":"1S6Wj34C9uOII3Um-OWXlMO-ztX2FTl21","timestamp":1574791677538},{"file_id":"18zcR7miQsixNzxSzeY4NjUAqznVL-9aw","timestamp":1574787092815},{"file_id":"11Z1YxyMnx7r9zk5KucF7zgdrDv7__Q9f","timestamp":1574784450971},{"file_id":"1GyjxF5uw647LcmGSXy4kifN_i6HeGFsQ","timestamp":1574781597038},{"file_id":"1Uaoy1W9KJZ8lWZsmdA-3fLWRb5EAzlYp","timestamp":1574780594576},{"file_id":"1QUNQ432HvAKEa9_3xgxTOJomZdxmi6sQ","timestamp":1574772959831},{"file_id":"12xNbP_2KivhaBh9rhBFD9vqJp56Yjc8T","timestamp":1574770099497},{"file_id":"1inQbH3sHy1w2AnOd8FM8yDJ4ltYO3Rr-","timestamp":1574769467131},{"file_id":"https://github.com/hochaeAidl/ai-middle-course/blob/master/code_templete.ipynb","timestamp":1574766303250}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YUbrsmxWf04y","colab_type":"text"},"source":["# **실습 3-2 : Char-RNN**\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZQKizrb-Y1er"},"source":["## **Import Module**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0trJmd6DjqBZ","outputId":"b9a10883-6e67-460c-d35a-5157f3de941e","executionInfo":{"status":"ok","timestamp":1575973778250,"user_tz":-540,"elapsed":2275,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["%tensorflow_version 2.x\n","\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","tf.__version__"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'2.0.0'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"wOiaPQ3xgJQC","colab_type":"text"},"source":["## **DataSet**"]},{"cell_type":"markdown","metadata":{"id":"aD28IHuegUxp","colab_type":"text"},"source":["### 학습할 문장 만들기"]},{"cell_type":"code","metadata":{"id":"cC8AS287Hz-9","colab_type":"code","outputId":"fb3219ee-789d-46ea-9299-e18b3ed7c1d0","executionInfo":{"status":"ok","timestamp":1575973778252,"user_tz":-540,"elapsed":2267,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"source":["# 학습할 문장\n","sentence = ( \"Rhyme, alliteration, assonance and consonance are ways of creating repetitive patterns of sound. They may be used as an independent structural element in a poem, to reinforce rhythmic patterns, or as an ornamental element.[66] They can also carry a meaning separate from the repetitive sound patterns created. For example, Chaucer used heavy alliteration to mock Old English verse and to paint a character as archaic.[67]\"\n","\n","\"Rhyme consists of identical (hard-rhyme) or similar (soft-rhyme) sounds placed at the ends of lines or at predictable locations within lines (internal rhyme). Languages vary in the richness of their rhyming structures; Italian, for example, has a rich rhyming structure permitting maintenance of a limited set of rhymes throughout a lengthy poem. The richness results from word endings that follow regular forms. English, with its irregular word endings adopted from other languages, is less rich in rhyme.[68] The degree of richness of a languages rhyming structures plays a substantial role in determining what poetic forms are commonly used in that language.[69]\"\n","\n","\"Alliteration is the repetition of letters or letter-sounds at the beginning of two or more words immediately succeeding each other, or at short intervals; or the recurrence of the same letter in accented parts of words. Alliteration and assonance played a key role in structuring early Germanic, Norse and Old English forms of poetry. The alliterative patterns of early Germanic poetry interweave meter and alliteration as a key part of their structure, so that the metrical pattern determines when the listener expects instances of alliteration to occur. This can be compared to an ornamental use of alliteration in most Modern European poetry, where alliterative patterns are not formal or carried through full stanzas. Alliteration is particularly useful in languages with less rich rhyming structures.\"\n","\n","\"Assonance, where the use of similar vowel sounds within a word rather than similar sounds at the beginning or end of a word, was widely used in skaldic poetry but goes back to the Homeric epic.[70] Because verbs carry much of the pitch in the English language, assonance can loosely evoke the tonal elements of Chinese poetry and so is useful in translating Chinese poetry.[71] Consonance occurs where a consonant sound is repeated throughout a sentence without putting the sound only at the front of a word. Consonance provokes a more subtle effect than alliteration and so is less useful as a structural element.[69]\"\n",")\n","# sentence = (\"if you want to build a ship, don't drum up people together to \"\n","#             \"collect wood and don't assign them tasks and work, but rather \"\n","#             \"teach them to long for the endless immensity of the sea.\" )\n","print (\"FOLLOWING IS OUR TRAINING SEQUENCE:\")\n","print (sentence)\n","print (\"Length of 'test sentence' is %s\" %len(sentence))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["FOLLOWING IS OUR TRAINING SEQUENCE:\n","Rhyme, alliteration, assonance and consonance are ways of creating repetitive patterns of sound. They may be used as an independent structural element in a poem, to reinforce rhythmic patterns, or as an ornamental element.[66] They can also carry a meaning separate from the repetitive sound patterns created. For example, Chaucer used heavy alliteration to mock Old English verse and to paint a character as archaic.[67]Rhyme consists of identical (hard-rhyme) or similar (soft-rhyme) sounds placed at the ends of lines or at predictable locations within lines (internal rhyme). Languages vary in the richness of their rhyming structures; Italian, for example, has a rich rhyming structure permitting maintenance of a limited set of rhymes throughout a lengthy poem. The richness results from word endings that follow regular forms. English, with its irregular word endings adopted from other languages, is less rich in rhyme.[68] The degree of richness of a languages rhyming structures plays a substantial role in determining what poetic forms are commonly used in that language.[69]Alliteration is the repetition of letters or letter-sounds at the beginning of two or more words immediately succeeding each other, or at short intervals; or the recurrence of the same letter in accented parts of words. Alliteration and assonance played a key role in structuring early Germanic, Norse and Old English forms of poetry. The alliterative patterns of early Germanic poetry interweave meter and alliteration as a key part of their structure, so that the metrical pattern determines when the listener expects instances of alliteration to occur. This can be compared to an ornamental use of alliteration in most Modern European poetry, where alliterative patterns are not formal or carried through full stanzas. Alliteration is particularly useful in languages with less rich rhyming structures.Assonance, where the use of similar vowel sounds within a word rather than similar sounds at the beginning or end of a word, was widely used in skaldic poetry but goes back to the Homeric epic.[70] Because verbs carry much of the pitch in the English language, assonance can loosely evoke the tonal elements of Chinese poetry and so is useful in translating Chinese poetry.[71] Consonance occurs where a consonant sound is repeated throughout a sentence without putting the sound only at the front of a word. Consonance provokes a more subtle effect than alliteration and so is less useful as a structural element.[69]\n","Length of 'test sentence' is 2509\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O0oDy0CggOEm","colab_type":"text"},"source":["### 입력 문자열과 타겟 문자 준비"]},{"cell_type":"code","metadata":{"id":"R7d2RtBMDI6a","colab_type":"code","outputId":"85e4c94c-775c-4c03-c97e-76542c7a991c","executionInfo":{"status":"ok","timestamp":1575973778253,"user_tz":-540,"elapsed":2261,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":145}},"source":["# make charater dictionary \n","char_set = list(set(sentence))\n","char_dic = {w: i for i, w in enumerate(char_set)}\n","print (\"CHARACTERS: \")\n","print (len(char_set))\n","print (char_set)\n","print (\"DICTIONARY: \")\n","print (len(char_dic))\n","print (char_dic)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["CHARACTERS: \n","53\n","['b', 'F', 'R', 'r', 'H', 'f', 'e', '[', 'k', '7', '9', 'M', 'C', 's', 'v', ')', '8', 't', 'x', 'u', 'I', ',', 'B', '1', 'd', 'o', '6', 'l', 'y', '0', 'a', 'G', 'E', 'p', '.', ']', 'N', 'w', 'n', 'A', '-', '(', 'T', 'm', 'h', 'g', ' ', 'i', 'L', 'z', 'O', 'c', ';']\n","DICTIONARY: \n","53\n","{'b': 0, 'F': 1, 'R': 2, 'r': 3, 'H': 4, 'f': 5, 'e': 6, '[': 7, 'k': 8, '7': 9, '9': 10, 'M': 11, 'C': 12, 's': 13, 'v': 14, ')': 15, '8': 16, 't': 17, 'x': 18, 'u': 19, 'I': 20, ',': 21, 'B': 22, '1': 23, 'd': 24, 'o': 25, '6': 26, 'l': 27, 'y': 28, '0': 29, 'a': 30, 'G': 31, 'E': 32, 'p': 33, '.': 34, ']': 35, 'N': 36, 'w': 37, 'n': 38, 'A': 39, '-': 40, '(': 41, 'T': 42, 'm': 43, 'h': 44, 'g': 45, ' ': 46, 'i': 47, 'L': 48, 'z': 49, 'O': 50, 'c': 51, ';': 52}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1LXjO9txq9nR","colab_type":"code","outputId":"87395fd0-8c72-4377-bd34-f44f334cb98c","executionInfo":{"status":"ok","timestamp":1575973778254,"user_tz":-540,"elapsed":2250,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["data_dim    = len(char_set) # train data X:input\n","num_classes = len(char_set) # trian data Y:target\n","sequence_length = 10        # any arbitrary number\n","print ('data_dim : %d' %data_dim)\n","print ('num_classes : %d' %num_classes)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["data_dim : 53\n","num_classes : 53\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DGnO0NygDI6M","colab_type":"code","outputId":"23e8317c-b8f3-4ef9-9f77-e563c0834859","executionInfo":{"status":"ok","timestamp":1575973778255,"user_tz":-540,"elapsed":2243,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["dataX = []  # input sequence list\n","dataY = []  # target sequence list \n","\n","# we will make 170 sequences \n","for i in range(0, len(sentence) - sequence_length):\n","    # 10 characters = 1 training sequence\n","    x_str = sentence[i : i+sequence_length]\n","    y_str = sentence[i+1 : i+sequence_length+1]\n","    # convert x, y str to index(int)\n","    x_idx = [char_dic[c] for c in x_str] \n","    y_idx = [char_dic[c] for c in y_str] \n","    # append to dataset list\n","    dataX.append(x_idx)\n","    dataY.append(y_idx)\n","\n","    # monitoring\n","    if i<5:\n","        print (\"[%3d/%3d] [%s]=>[%s]\" % (i, len(sentence), x_str, y_str))\n","        print (\"%s%s=>%s\" % (' '*10, x_idx, y_idx))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[  0/2509] [Rhyme, all]=>[hyme, alli]\n","          [2, 44, 28, 43, 6, 21, 46, 30, 27, 27]=>[44, 28, 43, 6, 21, 46, 30, 27, 27, 47]\n","[  1/2509] [hyme, alli]=>[yme, allit]\n","          [44, 28, 43, 6, 21, 46, 30, 27, 27, 47]=>[28, 43, 6, 21, 46, 30, 27, 27, 47, 17]\n","[  2/2509] [yme, allit]=>[me, allite]\n","          [28, 43, 6, 21, 46, 30, 27, 27, 47, 17]=>[43, 6, 21, 46, 30, 27, 27, 47, 17, 6]\n","[  3/2509] [me, allite]=>[e, alliter]\n","          [43, 6, 21, 46, 30, 27, 27, 47, 17, 6]=>[6, 21, 46, 30, 27, 27, 47, 17, 6, 3]\n","[  4/2509] [e, alliter]=>[, allitera]\n","          [6, 21, 46, 30, 27, 27, 47, 17, 6, 3]=>[21, 46, 30, 27, 27, 47, 17, 6, 3, 30]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"59y_7OfhvMpY","colab_type":"text"},"source":["### 입력문자 list를 3-dim로, 타겟문자 list를 2-dim로 변환  "]},{"cell_type":"code","metadata":{"id":"Obs8_hpYv1gd","colab_type":"code","outputId":"2383cdde-4cdd-46da-b1f1-4c8a41cba701","executionInfo":{"status":"ok","timestamp":1575973778257,"user_tz":-540,"elapsed":2237,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# input tensor 생성 \n","X = np.array(dataX)                                      # ndarray(170,10)<-[170,10]\n","# convert list sequence to 3dim array(one-hot coding)\n","sequences = [tf.keras.utils.to_categorical(\n","                 x, num_classes = data_dim) for x in X]  # list[170,10,25]\n","X = np.array(sequences)                                  # ndarray(170,10,25)\n","\n","# Target tensor 생성\n","#y = np.array(dataY)[:,-1]                                # ndarray(170,)<-[170,10]\n","y = np.array(dataY)                                       # ndarray(170,10)<-[170,10]\n","# 만약 :. -1로 끊어진 경우에는 하나의 캐릭터를 빼내고 그걸로 학습. 근데 전체로 할때는 한 캐릭터 뿐만 아니라 중간 값도 맞기를 원하기 때문에 더 좋은 결과가 나온다.\n","# convert the vector to 2-dim \n","y = tf.keras.utils.to_categorical( \n","                  y, num_classes = data_dim)             # (170,25) -> (170,10,25)\n","\n","print (y.shape) # (170,10,25)\n","#print (y[0])    # t -> one-hot ,  (if you wan't')\n","\n","\n","# 마지막 캐릭터 하나만 비교하는 것이 아니라 캐릭터 전체를 비교해줄 필ㅇ요가 있고, "],"execution_count":6,"outputs":[{"output_type":"stream","text":["(2499, 10, 53)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EY3zfzjlgbYj","colab_type":"text"},"source":["## **Model**"]},{"cell_type":"markdown","metadata":{"id":"PUqUyRtngdUC","colab_type":"text"},"source":["### Define"]},{"cell_type":"code","metadata":{"id":"vCAvqXiiDI6X","colab_type":"code","outputId":"7eab14e9-a762-455d-a783-722222083d48","executionInfo":{"status":"ok","timestamp":1575973783596,"user_tz":-540,"elapsed":7569,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":341}},"source":["# Sequenctial model define: LSTM + Dense\n","hidden_dim = 150\n","model=tf.keras.models.Sequential()\n","model.add(tf.keras.layers.LSTM(hidden_dim,\n","          input_shape=(sequence_length,num_classes),\n","          return_sequences=True)) # (bs,10,hidden_dim)<-(bs,hidden_dim) 시퀀스에서까지의 분포를 다 학습한다. 처음의 코드는 전체적으로 볼 때 하나의 코드였ㄷ고\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.LSTM(hidden_dim, return_sequences=True))\n","model.add(tf.keras.layers.LSTM(hidden_dim, return_sequences=True))\n","model.add(tf.keras.layers.Dense(data_dim, activation='softmax'))\n","model.summary()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm (LSTM)                  (None, 10, 150)           122400    \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 10, 150)           600       \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 10, 150)           180600    \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 10, 150)           180600    \n","_________________________________________________________________\n","dense (Dense)                (None, 10, 53)            8003      \n","=================================================================\n","Total params: 492,203\n","Trainable params: 491,903\n","Non-trainable params: 300\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zMat0-hSzeq_"},"source":["### Compile"]},{"cell_type":"code","metadata":{"id":"P3VyAq7gzkuA","colab_type":"code","colab":{}},"source":["model.compile(loss ='categorical_crossentropy', # one-hot coding\n","              optimizer='adam', \n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n9BG8hNUgo0n","colab_type":"text"},"source":["### Fit"]},{"cell_type":"markdown","metadata":{"id":"7KbIreYLd6gj","colab_type":"text"},"source":["Epoch 1000/1000   \n","170/170 [==============================] - 0s 235us/sample - loss: 0.0012 - accuracy: 1.0000    \n","CPU times: user 48.3 s, sys: 6.54 s, total: 54.8 s    \n","Wall time: 47.8 s (@Notebook Setting/GPU)"]},{"cell_type":"code","metadata":{"id":"-_idMKYwKVqX","colab_type":"code","outputId":"ca7f10d7-ece1-4ba0-8cc3-3280e7feef8c","executionInfo":{"status":"error","timestamp":1575974079364,"user_tz":-540,"elapsed":303324,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%%time\n","model.fit(X,            # X.shape : (170, 10, 25)\n","          y,            # y.shape : (170, 10, 25)\n","          epochs=400)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Train on 2499 samples\n","Epoch 1/400\n","2499/2499 [==============================] - 11s 4ms/sample - loss: 3.0828 - accuracy: 0.1632\n","Epoch 2/400\n","2499/2499 [==============================] - 2s 750us/sample - loss: 2.4862 - accuracy: 0.2956\n","Epoch 3/400\n","2499/2499 [==============================] - 2s 741us/sample - loss: 1.9736 - accuracy: 0.4386\n","Epoch 4/400\n","2499/2499 [==============================] - 2s 746us/sample - loss: 1.6152 - accuracy: 0.5378\n","Epoch 5/400\n","2499/2499 [==============================] - 2s 747us/sample - loss: 1.3566 - accuracy: 0.6070\n","Epoch 6/400\n","2499/2499 [==============================] - 2s 739us/sample - loss: 1.1646 - accuracy: 0.6613\n","Epoch 7/400\n","2499/2499 [==============================] - 2s 749us/sample - loss: 1.0170 - accuracy: 0.7057\n","Epoch 8/400\n","2499/2499 [==============================] - 2s 752us/sample - loss: 0.9044 - accuracy: 0.7348\n","Epoch 9/400\n","2499/2499 [==============================] - 2s 740us/sample - loss: 0.8218 - accuracy: 0.7535\n","Epoch 10/400\n","2499/2499 [==============================] - 2s 733us/sample - loss: 0.7665 - accuracy: 0.7651\n","Epoch 11/400\n","2499/2499 [==============================] - 2s 750us/sample - loss: 0.7185 - accuracy: 0.7761\n","Epoch 12/400\n","2499/2499 [==============================] - 2s 740us/sample - loss: 0.6868 - accuracy: 0.7815\n","Epoch 13/400\n","2499/2499 [==============================] - 2s 753us/sample - loss: 0.6646 - accuracy: 0.7862\n","Epoch 14/400\n","2499/2499 [==============================] - 2s 753us/sample - loss: 0.6518 - accuracy: 0.7878\n","Epoch 15/400\n","2499/2499 [==============================] - 2s 753us/sample - loss: 0.6303 - accuracy: 0.7896\n","Epoch 16/400\n","2499/2499 [==============================] - 2s 753us/sample - loss: 0.6178 - accuracy: 0.7924\n","Epoch 17/400\n","2499/2499 [==============================] - 2s 740us/sample - loss: 0.6085 - accuracy: 0.7920\n","Epoch 18/400\n","2499/2499 [==============================] - 2s 740us/sample - loss: 0.5989 - accuracy: 0.7949\n","Epoch 19/400\n","2499/2499 [==============================] - 2s 737us/sample - loss: 0.5879 - accuracy: 0.7945\n","Epoch 20/400\n","2499/2499 [==============================] - 2s 751us/sample - loss: 0.5856 - accuracy: 0.7939\n","Epoch 21/400\n","2499/2499 [==============================] - 2s 747us/sample - loss: 0.5803 - accuracy: 0.7944\n","Epoch 22/400\n","2499/2499 [==============================] - 2s 735us/sample - loss: 0.5749 - accuracy: 0.7972\n","Epoch 23/400\n","2499/2499 [==============================] - 2s 746us/sample - loss: 0.5715 - accuracy: 0.7947\n","Epoch 24/400\n","2499/2499 [==============================] - 2s 746us/sample - loss: 0.5696 - accuracy: 0.7961\n","Epoch 25/400\n","2499/2499 [==============================] - 2s 746us/sample - loss: 0.5654 - accuracy: 0.7983\n","Epoch 26/400\n","2499/2499 [==============================] - 2s 739us/sample - loss: 0.5639 - accuracy: 0.7949\n","Epoch 27/400\n","2499/2499 [==============================] - 2s 736us/sample - loss: 0.5585 - accuracy: 0.7954\n","Epoch 28/400\n","2499/2499 [==============================] - 2s 749us/sample - loss: 0.5564 - accuracy: 0.7983\n","Epoch 29/400\n","2499/2499 [==============================] - 2s 737us/sample - loss: 0.5512 - accuracy: 0.7973\n","Epoch 30/400\n","2499/2499 [==============================] - 2s 732us/sample - loss: 0.5517 - accuracy: 0.7987\n","Epoch 31/400\n","2499/2499 [==============================] - 2s 735us/sample - loss: 0.5513 - accuracy: 0.7970\n","Epoch 32/400\n","2499/2499 [==============================] - 2s 735us/sample - loss: 0.5495 - accuracy: 0.7983\n","Epoch 33/400\n","2499/2499 [==============================] - 2s 750us/sample - loss: 0.5442 - accuracy: 0.7984\n","Epoch 34/400\n","2499/2499 [==============================] - 2s 764us/sample - loss: 0.5472 - accuracy: 0.7972\n","Epoch 35/400\n","2499/2499 [==============================] - 2s 742us/sample - loss: 0.5464 - accuracy: 0.7992\n","Epoch 36/400\n","2499/2499 [==============================] - 2s 745us/sample - loss: 0.5416 - accuracy: 0.7982\n","Epoch 37/400\n","2499/2499 [==============================] - 2s 746us/sample - loss: 0.5423 - accuracy: 0.7967\n","Epoch 38/400\n","2499/2499 [==============================] - 2s 738us/sample - loss: 0.5378 - accuracy: 0.7990\n","Epoch 39/400\n","2499/2499 [==============================] - 2s 753us/sample - loss: 0.5365 - accuracy: 0.7981\n","Epoch 40/400\n","2499/2499 [==============================] - 2s 735us/sample - loss: 0.5386 - accuracy: 0.7986\n","Epoch 41/400\n","2499/2499 [==============================] - 2s 740us/sample - loss: 0.5352 - accuracy: 0.7994\n","Epoch 42/400\n","2499/2499 [==============================] - 2s 738us/sample - loss: 0.5358 - accuracy: 0.7965\n","Epoch 43/400\n","2499/2499 [==============================] - 2s 739us/sample - loss: 0.5351 - accuracy: 0.7973\n","Epoch 44/400\n","2499/2499 [==============================] - 2s 756us/sample - loss: 0.5381 - accuracy: 0.7988\n","Epoch 45/400\n","2499/2499 [==============================] - 2s 752us/sample - loss: 0.5336 - accuracy: 0.8000\n","Epoch 46/400\n","2499/2499 [==============================] - 2s 754us/sample - loss: 0.5314 - accuracy: 0.7985\n","Epoch 47/400\n","2499/2499 [==============================] - 2s 765us/sample - loss: 0.5302 - accuracy: 0.7988\n","Epoch 48/400\n","2499/2499 [==============================] - 2s 747us/sample - loss: 0.5300 - accuracy: 0.7987\n","Epoch 49/400\n","2499/2499 [==============================] - 2s 744us/sample - loss: 0.5282 - accuracy: 0.7993\n","Epoch 50/400\n","2499/2499 [==============================] - 2s 756us/sample - loss: 0.5295 - accuracy: 0.8005\n","Epoch 51/400\n","2499/2499 [==============================] - 2s 749us/sample - loss: 0.5301 - accuracy: 0.7961\n","Epoch 52/400\n","2499/2499 [==============================] - 2s 746us/sample - loss: 0.5239 - accuracy: 0.7988\n","Epoch 53/400\n","2499/2499 [==============================] - 2s 745us/sample - loss: 0.5258 - accuracy: 0.7998\n","Epoch 54/400\n","2499/2499 [==============================] - 2s 746us/sample - loss: 0.5257 - accuracy: 0.7992\n","Epoch 55/400\n","2499/2499 [==============================] - 2s 735us/sample - loss: 0.5235 - accuracy: 0.7999\n","Epoch 56/400\n","2499/2499 [==============================] - 2s 756us/sample - loss: 0.5255 - accuracy: 0.7985\n","Epoch 57/400\n","2499/2499 [==============================] - 2s 746us/sample - loss: 0.5219 - accuracy: 0.7989\n","Epoch 58/400\n","2499/2499 [==============================] - 2s 740us/sample - loss: 0.5233 - accuracy: 0.7998\n","Epoch 59/400\n","2499/2499 [==============================] - 2s 737us/sample - loss: 0.5258 - accuracy: 0.7990\n","Epoch 60/400\n","2499/2499 [==============================] - 2s 752us/sample - loss: 0.5212 - accuracy: 0.7982\n","Epoch 61/400\n","2499/2499 [==============================] - 2s 738us/sample - loss: 0.5209 - accuracy: 0.7994\n","Epoch 62/400\n","2499/2499 [==============================] - 2s 743us/sample - loss: 0.5196 - accuracy: 0.8000\n","Epoch 63/400\n","2499/2499 [==============================] - 2s 737us/sample - loss: 0.5205 - accuracy: 0.7996\n","Epoch 64/400\n","2499/2499 [==============================] - 2s 745us/sample - loss: 0.5196 - accuracy: 0.8015\n","Epoch 65/400\n","2499/2499 [==============================] - 2s 743us/sample - loss: 0.5177 - accuracy: 0.7994\n","Epoch 66/400\n","2499/2499 [==============================] - 2s 764us/sample - loss: 0.5186 - accuracy: 0.8012\n","Epoch 67/400\n","2499/2499 [==============================] - 2s 734us/sample - loss: 0.5202 - accuracy: 0.7983\n","Epoch 68/400\n","2499/2499 [==============================] - 2s 756us/sample - loss: 0.5174 - accuracy: 0.8009\n","Epoch 69/400\n","2499/2499 [==============================] - 2s 741us/sample - loss: 0.5227 - accuracy: 0.7999\n","Epoch 70/400\n","2499/2499 [==============================] - 2s 751us/sample - loss: 0.5168 - accuracy: 0.8000\n","Epoch 71/400\n","2499/2499 [==============================] - 2s 741us/sample - loss: 0.5174 - accuracy: 0.7994\n","Epoch 72/400\n","2499/2499 [==============================] - 2s 751us/sample - loss: 0.5163 - accuracy: 0.8011\n","Epoch 73/400\n","2499/2499 [==============================] - 2s 744us/sample - loss: 0.5166 - accuracy: 0.8010\n","Epoch 74/400\n","2499/2499 [==============================] - 2s 729us/sample - loss: 0.5151 - accuracy: 0.8009\n","Epoch 75/400\n","2499/2499 [==============================] - 2s 755us/sample - loss: 0.5143 - accuracy: 0.8003\n","Epoch 76/400\n","2499/2499 [==============================] - 2s 742us/sample - loss: 0.5164 - accuracy: 0.7982\n","Epoch 77/400\n","2499/2499 [==============================] - 2s 748us/sample - loss: 0.5151 - accuracy: 0.7987\n","Epoch 78/400\n","2499/2499 [==============================] - 2s 738us/sample - loss: 0.5142 - accuracy: 0.8014\n","Epoch 79/400\n","2499/2499 [==============================] - 2s 726us/sample - loss: 0.5171 - accuracy: 0.8012\n","Epoch 80/400\n","2499/2499 [==============================] - 2s 741us/sample - loss: 0.5143 - accuracy: 0.8026\n","Epoch 81/400\n","2499/2499 [==============================] - 2s 762us/sample - loss: 0.5135 - accuracy: 0.8005\n","Epoch 82/400\n","2499/2499 [==============================] - 2s 768us/sample - loss: 0.5150 - accuracy: 0.8013\n","Epoch 83/400\n","2499/2499 [==============================] - 2s 759us/sample - loss: 0.5160 - accuracy: 0.8001\n","Epoch 84/400\n","2499/2499 [==============================] - 2s 773us/sample - loss: 0.5144 - accuracy: 0.8004\n","Epoch 85/400\n","2499/2499 [==============================] - 2s 765us/sample - loss: 0.5141 - accuracy: 0.8002\n","Epoch 86/400\n","2499/2499 [==============================] - 2s 774us/sample - loss: 0.5112 - accuracy: 0.8019\n","Epoch 87/400\n","2499/2499 [==============================] - 2s 730us/sample - loss: 0.5114 - accuracy: 0.8013\n","Epoch 88/400\n","2499/2499 [==============================] - 2s 735us/sample - loss: 0.5139 - accuracy: 0.7993\n","Epoch 89/400\n","2499/2499 [==============================] - 2s 753us/sample - loss: 0.5111 - accuracy: 0.8016\n","Epoch 90/400\n","2499/2499 [==============================] - 2s 730us/sample - loss: 0.5106 - accuracy: 0.8000\n","Epoch 91/400\n","2499/2499 [==============================] - 2s 756us/sample - loss: 0.5099 - accuracy: 0.8008\n","Epoch 92/400\n","2499/2499 [==============================] - 2s 759us/sample - loss: 0.5117 - accuracy: 0.8006\n","Epoch 93/400\n","2499/2499 [==============================] - 2s 752us/sample - loss: 0.5120 - accuracy: 0.7992\n","Epoch 94/400\n","2499/2499 [==============================] - 2s 741us/sample - loss: 0.5098 - accuracy: 0.7996\n","Epoch 95/400\n","2499/2499 [==============================] - 2s 740us/sample - loss: 0.5091 - accuracy: 0.8002\n","Epoch 96/400\n","2499/2499 [==============================] - 2s 745us/sample - loss: 0.5095 - accuracy: 0.8014\n","Epoch 97/400\n","2499/2499 [==============================] - 2s 736us/sample - loss: 0.5085 - accuracy: 0.8017\n","Epoch 98/400\n","2499/2499 [==============================] - 2s 737us/sample - loss: 0.5092 - accuracy: 0.8012\n","Epoch 99/400\n","2499/2499 [==============================] - 2s 741us/sample - loss: 0.5072 - accuracy: 0.8021\n","Epoch 100/400\n","2499/2499 [==============================] - 2s 743us/sample - loss: 0.5072 - accuracy: 0.8017\n","Epoch 101/400\n","2499/2499 [==============================] - 2s 744us/sample - loss: 0.5083 - accuracy: 0.8007\n","Epoch 102/400\n","2499/2499 [==============================] - 2s 758us/sample - loss: 0.5070 - accuracy: 0.8023\n","Epoch 103/400\n","2499/2499 [==============================] - 2s 734us/sample - loss: 0.5058 - accuracy: 0.8012\n","Epoch 104/400\n","2499/2499 [==============================] - 2s 740us/sample - loss: 0.5078 - accuracy: 0.8011\n","Epoch 105/400\n","2499/2499 [==============================] - 2s 744us/sample - loss: 0.5070 - accuracy: 0.8028\n","Epoch 106/400\n","2499/2499 [==============================] - 2s 754us/sample - loss: 0.5054 - accuracy: 0.8026\n","Epoch 107/400\n","2499/2499 [==============================] - 2s 758us/sample - loss: 0.5068 - accuracy: 0.8010\n","Epoch 108/400\n","2499/2499 [==============================] - 2s 754us/sample - loss: 0.5049 - accuracy: 0.8015\n","Epoch 109/400\n","2499/2499 [==============================] - 2s 740us/sample - loss: 0.5050 - accuracy: 0.8013\n","Epoch 110/400\n","2499/2499 [==============================] - 2s 752us/sample - loss: 0.5065 - accuracy: 0.8001\n","Epoch 111/400\n","2499/2499 [==============================] - 2s 735us/sample - loss: 0.5047 - accuracy: 0.8018\n","Epoch 112/400\n","2499/2499 [==============================] - 2s 751us/sample - loss: 0.5047 - accuracy: 0.8035\n","Epoch 113/400\n","2499/2499 [==============================] - 2s 740us/sample - loss: 0.5032 - accuracy: 0.8030\n","Epoch 114/400\n","2499/2499 [==============================] - 2s 735us/sample - loss: 0.5043 - accuracy: 0.8038\n","Epoch 115/400\n","2499/2499 [==============================] - 2s 742us/sample - loss: 0.5034 - accuracy: 0.8014\n","Epoch 116/400\n","2499/2499 [==============================] - 2s 759us/sample - loss: 0.5026 - accuracy: 0.8024\n","Epoch 117/400\n","2499/2499 [==============================] - 2s 747us/sample - loss: 0.5034 - accuracy: 0.8042\n","Epoch 118/400\n","2499/2499 [==============================] - 2s 739us/sample - loss: 0.5041 - accuracy: 0.8030\n","Epoch 119/400\n","2499/2499 [==============================] - 2s 754us/sample - loss: 0.5023 - accuracy: 0.8010\n","Epoch 120/400\n","2499/2499 [==============================] - 2s 755us/sample - loss: 0.5033 - accuracy: 0.8031\n","Epoch 121/400\n","2499/2499 [==============================] - 2s 754us/sample - loss: 0.5023 - accuracy: 0.8017\n","Epoch 122/400\n","2499/2499 [==============================] - 2s 743us/sample - loss: 0.5023 - accuracy: 0.8012\n","Epoch 123/400\n","2499/2499 [==============================] - 2s 758us/sample - loss: 0.5029 - accuracy: 0.8032\n","Epoch 124/400\n","2499/2499 [==============================] - 2s 765us/sample - loss: 0.5021 - accuracy: 0.8025\n","Epoch 125/400\n","2499/2499 [==============================] - 2s 787us/sample - loss: 0.5039 - accuracy: 0.8018\n","Epoch 126/400\n","2499/2499 [==============================] - 2s 755us/sample - loss: 0.5016 - accuracy: 0.8021\n","Epoch 127/400\n","2499/2499 [==============================] - 2s 753us/sample - loss: 0.5016 - accuracy: 0.8036\n","Epoch 128/400\n","2499/2499 [==============================] - 2s 755us/sample - loss: 0.5043 - accuracy: 0.8035\n","Epoch 129/400\n","2499/2499 [==============================] - 2s 724us/sample - loss: 0.5016 - accuracy: 0.8032\n","Epoch 130/400\n","2499/2499 [==============================] - 2s 743us/sample - loss: 0.5016 - accuracy: 0.8023\n","Epoch 131/400\n","2499/2499 [==============================] - 2s 750us/sample - loss: 0.5039 - accuracy: 0.8022\n","Epoch 132/400\n","2499/2499 [==============================] - 2s 746us/sample - loss: 0.5017 - accuracy: 0.8010\n","Epoch 133/400\n","2499/2499 [==============================] - 2s 759us/sample - loss: 0.5013 - accuracy: 0.8033\n","Epoch 134/400\n","2499/2499 [==============================] - 2s 758us/sample - loss: 0.5010 - accuracy: 0.8022\n","Epoch 135/400\n","2499/2499 [==============================] - 2s 759us/sample - loss: 0.5000 - accuracy: 0.8030\n","Epoch 136/400\n","2499/2499 [==============================] - 2s 762us/sample - loss: 0.4992 - accuracy: 0.8036\n","Epoch 137/400\n","2499/2499 [==============================] - 2s 765us/sample - loss: 0.5001 - accuracy: 0.8034\n","Epoch 138/400\n","2499/2499 [==============================] - 2s 745us/sample - loss: 0.5022 - accuracy: 0.8022\n","Epoch 139/400\n","2499/2499 [==============================] - 2s 754us/sample - loss: 0.5003 - accuracy: 0.8017\n","Epoch 140/400\n","2499/2499 [==============================] - 2s 751us/sample - loss: 0.4991 - accuracy: 0.8038\n","Epoch 141/400\n","2499/2499 [==============================] - 2s 741us/sample - loss: 0.5014 - accuracy: 0.8026\n","Epoch 142/400\n","2499/2499 [==============================] - 2s 762us/sample - loss: 0.5035 - accuracy: 0.8032\n","Epoch 143/400\n","2499/2499 [==============================] - 2s 760us/sample - loss: 0.5005 - accuracy: 0.8027\n","Epoch 144/400\n","2499/2499 [==============================] - 2s 749us/sample - loss: 0.4992 - accuracy: 0.8043\n","Epoch 145/400\n","2499/2499 [==============================] - 2s 749us/sample - loss: 0.5012 - accuracy: 0.8037\n","Epoch 146/400\n","2499/2499 [==============================] - 2s 768us/sample - loss: 0.4995 - accuracy: 0.8036\n","Epoch 147/400\n","2499/2499 [==============================] - 2s 745us/sample - loss: 0.4987 - accuracy: 0.8057\n","Epoch 148/400\n","2499/2499 [==============================] - 2s 772us/sample - loss: 0.4996 - accuracy: 0.8036\n","Epoch 149/400\n","2499/2499 [==============================] - 2s 764us/sample - loss: 0.4982 - accuracy: 0.8024\n","Epoch 150/400\n","2499/2499 [==============================] - 2s 742us/sample - loss: 0.4977 - accuracy: 0.8050\n","Epoch 151/400\n","2499/2499 [==============================] - 2s 747us/sample - loss: 0.4975 - accuracy: 0.8034\n","Epoch 152/400\n","2499/2499 [==============================] - 2s 749us/sample - loss: 0.4973 - accuracy: 0.8038\n","Epoch 153/400\n","2176/2499 [=========================>....] - ETA: 0s - loss: 0.4951 - accuracy: 0.8039"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-32c3c9e0f52f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.fit(X,            # X.shape : (170, 10, 25)\\n          y,            # y.shape : (170, 10, 25)\\n          epochs=400)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    172\u001b[0m             batch_end=step * batch_size + current_batch_size)\n\u001b[1;32m    173\u001b[0m       \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m       \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[0;34m(self, step, mode, size)\u001b[0m\n\u001b[1;32m    699\u001b[0m         self.callbacks._call_batch_hook(\n\u001b[1;32m    700\u001b[0m             mode, 'end', step, batch_logs)\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' - %s:'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m           \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"v7Bf90ATg98K","colab_type":"text"},"source":["## **Generate Text:** analysis"]},{"cell_type":"markdown","metadata":{"id":"VcGTERphhBNs","colab_type":"text"},"source":["### Related Module Import"]},{"cell_type":"markdown","metadata":{"id":"6uBnrJJe0fgz","colab_type":"text"},"source":["### Define generate_seq function"]},{"cell_type":"code","metadata":{"id":"OFq9I4aCDI6k","colab_type":"code","colab":{}},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# generate a sequence of characters with a language model\n","def generate_seq(model, char_dic, seq_length, seed_text, n_chars):\n","\tin_text = seed_text\t\t\t\t\t\t\t\t\t# seed text + generated text\n","\t# generate a fixed number of characters\n","\tfor i in range(n_chars):\n","\t\t# encode the characters as integers\n","\t\tencoded = [char_dic[char] for char in in_text]\n","\t\t# truncate sequences to a fixed length\n","\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n","\t\t# one hot encode\n","\t\tencoded = tf.keras.utils.to_categorical(encoded, num_classes=len(char_dic))\n","\t\t#encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n","\t\t# predict character\n","\t\tyhat = model.predict_classes(encoded, verbose=0)\n","\t\t# reverse map integer to character\n","\t\tfor char, index in char_dic.items():\n","\t\t\tif index == yhat[0][-1]:\n","\t\t\t\tbreak\n","\t\t# append to input\n","\t\tin_text += char\n","\treturn in_text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2_tIRa7h1PUA"},"source":["### Generate a sequence of characters"]},{"cell_type":"markdown","metadata":{"id":"hhgIpBwG06uc","colab_type":"text"},"source":["학습문장: if you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea."]},{"cell_type":"code","metadata":{"id":"xC3D8JPW1O0Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":325},"outputId":"d2ef0256-7b71-40a3-b7d6-333e15c4b6a0","executionInfo":{"status":"ok","timestamp":1575974485239,"user_tz":-540,"elapsed":68458,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}}},"source":["# test start of rhyme\n","print('Trigger characters: \"123 \"\\nResult: \\n\"{}\"'.format( \n","      generate_seq(model, char_dic, sequence_length, 'aaaaaaaaaaaaaaaaaaaaaaa', 200)))\n","# test mid-line\n","print('\\nTrigger characters: \"d\"\\nResult: \\n\"{}\"'.format(\n","      generate_seq(model, char_dic, sequence_length, 'd', 200)))\n","# test not in original\n","print('\\nTrigger characters: \"aabbcr than\"\\nResult: \"{}\"'.format(\n","      generate_seq(model, char_dic, sequence_length, 'hi', 200)))\n","print('\\nTrigger characters: \"the recurrence of the same lette\"\\nResult: \"{}\"'.format(\n","      generate_seq(model, char_dic, sequence_length, 'the recurrence of the same lette', 200)))\n","\n","print('\\nTrigger characters: \"r at predic\"\\nResult: \"{}\"'.format(\n","      generate_seq(model, char_dic, sequence_length, 'r at predic', 800)))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Trigger characters: \"123 \"\n","Result: \n","\"aaaaaaaaaaaaaaaaaaaaaaanennds of lines or at predictable locations within lines (internal rhyme). Languages vary in the richness of their structures plays a substantial role in determining what poetic forms are commonly use\"\n","\n","Trigger characters: \"d\"\n","Result: \n","\"dcokeary used in that language.[69]Alliteration is particularly useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in translating Chinese poetry and so\"\n","\n","Trigger characters: \"aabbcr than\"\n","Result: \"hico the recurrence of the same letter in accented parts of words. Alliteration is particularly useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in tr\"\n","\n","Trigger characters: \"the recurrence of the same lette\"\n","Result: \"the recurrence of the same letter in accented parts of words. Alliteration is particularly useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is \"\n","\n","Trigger characters: \"r at predic\"\n","Result: \"r at predictable locations within lines (internal rhyme). Languages vary in the richness of their structures plays a substantial role in determining what poetic forms are commonly used in that language.[69]Alliteration is particularly useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in translating Chinese poetry and so is useful in tr\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X58HIiOvpuG3","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","print('\\nTrigger characters: \"aabbcr than\"\\nResult: \"{}\"'.format(\n","      generate_seq(model, char_dic, sequence_length, 'ports', 200)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnaUZbi_hIHw","colab_type":"text"},"source":["## **실습과제**"]},{"cell_type":"markdown","metadata":{"id":"07YM3eLIWxSY","colab_type":"text"},"source":["### 과제 1. 학습문장을 Keras Documents에서 2000자 정도 가져다 넣어서 실행해 보자('.'뒤에'\\n'도 넣어주자) \n","#### -- accuracy가 적당한가 확인하자.\n","#### -- 모델을 강화 하기 위해 `hidden_dim ='값을 150으로 높히고, \n","#### -- `#`로 막혀 있는 `BatchNormalization()`을 동작시켜 보자\n","#### -- `Trigger characters`를 다양하게 바꿔보자 \n","#### -- 결과를 확인하고, 분석해 보자:\n"," \n"]},{"cell_type":"markdown","metadata":{"id":"4MH6SV-9YS_H","colab_type":"text"},"source":["위에 있는 것들을 다 종합해본 결과 accuracy의 경우에는 약 80 퍼센트 선에서 진동하는 것을 확인할 수 있었다.\n","\n","관찰결과 짧은 단어나 낱개의 알파벳의 경우에는 비교적 정상적으로 문장을 출력하는 것을 확인할 수 있었다. \n","\n","aaaaaaaaaaaa와 같은 이상한 시작의 경우에는 처음 단어는 제대로 뽑아내지 못했으나 그 이후부터는 비교적 제대로 뽑아내는 것을 확인할 수 있었다.\n","\n","다만 hi로 시작하는 문장의 경우에는 이상한 점을 발견할 수 있었는데, 후반부에 계속해서 같은 문장이 반복하는 것이었다. \n","\n","이는 가장 마지막에 문장의 중간 부분을 테스트한 경우에도 발견됐는데, 이러한 문제점이 공통적으로 발견되는 지점은 so is useful in 이라는 문장이었다.\n","\n","추정하건데, useful in 이라는 맥락이 등장한 이후에 가장 자주 나온 것이 chinese라는 단어로 시작하는 문장이었고, 학습의 데이터가 제한적이다 보니 useful as와 같은 다른 가능성이 있음에도 가장 높은 가능성을 보이는 chinese 문장이 계속해서 반복적으로 나타난 것이 아닌가 추측해본다."]}]}