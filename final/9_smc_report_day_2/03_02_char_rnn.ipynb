{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_02_char_rnn.ipynb","provenance":[{"file_id":"1nkqVytRmAxDvYJwgoueea8kPBiCXirXw","timestamp":1574791737676},{"file_id":"1S6Wj34C9uOII3Um-OWXlMO-ztX2FTl21","timestamp":1574791677538},{"file_id":"18zcR7miQsixNzxSzeY4NjUAqznVL-9aw","timestamp":1574787092815},{"file_id":"11Z1YxyMnx7r9zk5KucF7zgdrDv7__Q9f","timestamp":1574784450971},{"file_id":"1GyjxF5uw647LcmGSXy4kifN_i6HeGFsQ","timestamp":1574781597038},{"file_id":"1Uaoy1W9KJZ8lWZsmdA-3fLWRb5EAzlYp","timestamp":1574780594576},{"file_id":"1QUNQ432HvAKEa9_3xgxTOJomZdxmi6sQ","timestamp":1574772959831},{"file_id":"12xNbP_2KivhaBh9rhBFD9vqJp56Yjc8T","timestamp":1574770099497},{"file_id":"1inQbH3sHy1w2AnOd8FM8yDJ4ltYO3Rr-","timestamp":1574769467131},{"file_id":"https://github.com/hochaeAidl/ai-middle-course/blob/master/code_templete.ipynb","timestamp":1574766303250}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YUbrsmxWf04y","colab_type":"text"},"source":["# **실습 3-2 : Char-RNN**\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZQKizrb-Y1er"},"source":["## **Import Module**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0trJmd6DjqBZ","outputId":"9fbbfb87-793b-4d35-9de0-e7964f1270ff","executionInfo":{"status":"ok","timestamp":1575972684246,"user_tz":-540,"elapsed":2289,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","tf.__version__"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'2.0.0'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"wOiaPQ3xgJQC","colab_type":"text"},"source":["## **DataSet**"]},{"cell_type":"markdown","metadata":{"id":"aD28IHuegUxp","colab_type":"text"},"source":["### 학습할 문장 만들기"]},{"cell_type":"code","metadata":{"id":"cC8AS287Hz-9","colab_type":"code","outputId":"11924498-542a-45c7-8944-505cdaa4f091","executionInfo":{"status":"ok","timestamp":1575972684247,"user_tz":-540,"elapsed":2283,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"source":["# 학습할 문장\n","sentence = (\"if you want to build a ship, don't drum up people together to \"\n","            \"collect wood and don't assign them tasks and work, but rather \"\n","            \"teach them to long for the endless immensity of the sea.\"\n","            \"if you want to build a ship, don't drum up people together to \"\n","            \"collect wood and don't assign them tasks and work, but rather \"\n","            \"teach them to long for the endless immensity of the sea.\")\n","print (\"FOLLOWING IS OUR TRAINING SEQUENCE:\")\n","print (sentence)\n","print (\"Length of 'test sentence' is %s\" %len(sentence))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["FOLLOWING IS OUR TRAINING SEQUENCE:\n","if you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.if you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n","Length of 'test sentence' is 360\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O0oDy0CggOEm","colab_type":"text"},"source":["### 입력 문자열과 타겟 문자 준비"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n1JO20hopHt-"},"source":["#### Char dictionary 만들기"]},{"cell_type":"code","metadata":{"id":"R7d2RtBMDI6a","colab_type":"code","outputId":"1e97d533-d31a-4f21-df02-c8b656de0f46","executionInfo":{"status":"ok","timestamp":1575972684248,"user_tz":-540,"elapsed":2275,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":145}},"source":["# make charater dictionary \n","char_set = list(set(sentence))\n","char_dic = {w: i for i, w in enumerate(char_set)}\n","print (\"CHARACTERS: \")\n","print (len(char_set))\n","print (char_set)\n","print (\"DICTIONARY: \")\n","print (len(char_dic))\n","print (char_dic)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["CHARACTERS: \n","25\n","['e', 'w', 'k', 'l', ',', 'b', 'h', 'u', '.', 'a', 'g', 'd', ' ', 's', 'i', 't', 'r', 'c', 'p', 'o', 'f', \"'\", 'n', 'y', 'm']\n","DICTIONARY: \n","25\n","{'e': 0, 'w': 1, 'k': 2, 'l': 3, ',': 4, 'b': 5, 'h': 6, 'u': 7, '.': 8, 'a': 9, 'g': 10, 'd': 11, ' ': 12, 's': 13, 'i': 14, 't': 15, 'r': 16, 'c': 17, 'p': 18, 'o': 19, 'f': 20, \"'\": 21, 'n': 22, 'y': 23, 'm': 24}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rXpRna2MpGvz"},"source":["#### 문자열"]},{"cell_type":"code","metadata":{"id":"1LXjO9txq9nR","colab_type":"code","outputId":"1475b521-f08a-4182-8349-b89d24eb50d9","executionInfo":{"status":"ok","timestamp":1575972684249,"user_tz":-540,"elapsed":2266,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["data_dim    = len(char_set) # train data X:input\n","num_classes = len(char_set) # trian data Y:target\n","sequence_length = 10        # any arbitrary number\n","print ('data_dim : %d' %data_dim)\n","print ('num_classes : %d' %num_classes)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["data_dim : 25\n","num_classes : 25\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DGnO0NygDI6M","colab_type":"code","outputId":"6057f273-6814-48a6-e76a-76eb785caf70","executionInfo":{"status":"ok","timestamp":1575972684249,"user_tz":-540,"elapsed":2259,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["dataX = []  # input sequence list\n","dataY = []  # target sequence list \n","\n","# we will make 170 sequences \n","for i in range(0, len(sentence) - sequence_length):\n","    # 10 characters = 1 training sequence\n","    x_str = sentence[i : i+sequence_length]\n","    y_str = sentence[i+1 : i+sequence_length+1]\n","    # convert x, y str to index(int)\n","    x_idx = [char_dic[c] for c in x_str] \n","    y_idx = [char_dic[c] for c in y_str] \n","    # append to dataset list\n","    dataX.append(x_idx)\n","    dataY.append(y_idx)\n","\n","    # monitoring\n","    if i<5:\n","        print (\"[%3d/%3d] [%s]=>[%s]\" % (i, len(sentence), x_str, y_str))\n","        print (\"%s%s=>%s\" % (' '*10, x_idx, y_idx))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[  0/360] [if you wan]=>[f you want]\n","          [14, 20, 12, 23, 19, 7, 12, 1, 9, 22]=>[20, 12, 23, 19, 7, 12, 1, 9, 22, 15]\n","[  1/360] [f you want]=>[ you want ]\n","          [20, 12, 23, 19, 7, 12, 1, 9, 22, 15]=>[12, 23, 19, 7, 12, 1, 9, 22, 15, 12]\n","[  2/360] [ you want ]=>[you want t]\n","          [12, 23, 19, 7, 12, 1, 9, 22, 15, 12]=>[23, 19, 7, 12, 1, 9, 22, 15, 12, 15]\n","[  3/360] [you want t]=>[ou want to]\n","          [23, 19, 7, 12, 1, 9, 22, 15, 12, 15]=>[19, 7, 12, 1, 9, 22, 15, 12, 15, 19]\n","[  4/360] [ou want to]=>[u want to ]\n","          [19, 7, 12, 1, 9, 22, 15, 12, 15, 19]=>[7, 12, 1, 9, 22, 15, 12, 15, 19, 12]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AA7IGA2nZwhf","colab_type":"code","outputId":"fb93dc6f-7210-4862-c367-5d0aacd34c42","executionInfo":{"status":"ok","timestamp":1575972684250,"user_tz":-540,"elapsed":2252,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["# check type and data\n","print('\\n')\n","print ((type(dataX)))\n","print (dataX[0])\n","print (dataX[1])\n","print (dataX[168])\n","print (dataX[169])\n","\n","print('\\n')\n","print ((type(dataY)))\n","print (dataY[0])\n","print (dataY[1])\n","print (dataY[168])\n","print (dataY[169])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["\n","\n","<class 'list'>\n","[14, 20, 12, 23, 19, 7, 12, 1, 9, 22]\n","[20, 12, 23, 19, 7, 12, 1, 9, 22, 15]\n","[12, 19, 20, 12, 15, 6, 0, 12, 13, 0]\n","[19, 20, 12, 15, 6, 0, 12, 13, 0, 9]\n","\n","\n","<class 'list'>\n","[20, 12, 23, 19, 7, 12, 1, 9, 22, 15]\n","[12, 23, 19, 7, 12, 1, 9, 22, 15, 12]\n","[19, 20, 12, 15, 6, 0, 12, 13, 0, 9]\n","[20, 12, 15, 6, 0, 12, 13, 0, 9, 8]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"59y_7OfhvMpY","colab_type":"text"},"source":["### 입력문자 list를 3-dim로, 타겟문자 list를 2-dim로 변환  "]},{"cell_type":"code","metadata":{"id":"Obs8_hpYv1gd","colab_type":"code","outputId":"e9026590-bf5c-42b1-df1f-65d96f65d214","executionInfo":{"status":"ok","timestamp":1575972684251,"user_tz":-540,"elapsed":2240,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# input tensor 생성 \n","X = np.array(dataX)                                      # ndarray(170,10)<-[170,10]\n","# convert list sequence to 3dim array(one-hot coding)\n","sequences = [tf.keras.utils.to_categorical(\n","                 x, num_classes = data_dim) for x in X]  # list[170,10,25]\n","X = np.array(sequences)                                  # ndarray(170,10,25)\n","\n","# Target tensor 생성\n","y = np.array(dataY)[:,-1]                                # ndarray(170,)<-[170,10]\n","# convert the vector to 2-dim \n","#y = np.array(dataY) \n","y = tf.keras.utils.to_categorical(\n","                  y, num_classes = data_dim)             # (170,25)\n","\n","print (y.shape) # (170, 25)\n","print (y[0])    # t -> one-hot ,  (if you wan't')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(350, 25)\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EY3zfzjlgbYj","colab_type":"text"},"source":["## **Model**"]},{"cell_type":"markdown","metadata":{"id":"PUqUyRtngdUC","colab_type":"text"},"source":["### Define"]},{"cell_type":"code","metadata":{"id":"vCAvqXiiDI6X","colab_type":"code","outputId":"15428e54-fa08-410a-f6f5-3a9511982cae","executionInfo":{"status":"ok","timestamp":1575972793746,"user_tz":-540,"elapsed":1275,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["# Sequenctial model define: LSTM + Dense\n","hidden_dim = 75\n","model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.LSTM(units=hidden_dim,\n","                               input_shape=(sequence_length,num_classes) ,return_sequences=True))\n","model.add(tf.keras.layers.LSTM(units=hidden_dim))\n","model.add(tf.keras.layers.Dense(data_dim, activation='softmax'))\n","model.summary()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_2 (LSTM)                (None, 10, 75)            30300     \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 75)                45300     \n","_________________________________________________________________\n","dense (Dense)                (None, 25)                1900      \n","=================================================================\n","Total params: 77,500\n","Trainable params: 77,500\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zMat0-hSzeq_"},"source":["### Compile"]},{"cell_type":"code","metadata":{"id":"P3VyAq7gzkuA","colab_type":"code","colab":{}},"source":["model.compile(loss='categorical_crossentropy', # one-hot coding\n","              optimizer='adam', \n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDGjjt7hZ0cB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n9BG8hNUgo0n","colab_type":"text"},"source":["### Fit"]},{"cell_type":"markdown","metadata":{"id":"7KbIreYLd6gj","colab_type":"text"},"source":["Epoch 1000/1000   \n","170/170 [==============================] - 0s 235us/sample - loss: 0.0012 - accuracy: 1.0000    \n","CPU times: user 48.3 s, sys: 6.54 s, total: 54.8 s    \n","Wall time: 47.8 s (@Notebook Setting/GPU)"]},{"cell_type":"code","metadata":{"id":"-_idMKYwKVqX","colab_type":"code","outputId":"3a0777a6-db3e-4c43-d481-4b1e1d4af26d","executionInfo":{"status":"ok","timestamp":1575972815842,"user_tz":-540,"elapsed":17816,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%%time\n","model.fit(X,            # X.shape : (170, 10, 25)\n","          y,            # y.shape : (170, 25)\n","          epochs=200)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Train on 350 samples\n","Epoch 1/200\n","350/350 [==============================] - 4s 11ms/sample - loss: 3.1959 - accuracy: 0.1200\n","Epoch 2/200\n","350/350 [==============================] - 0s 167us/sample - loss: 3.0338 - accuracy: 0.1886\n","Epoch 3/200\n","350/350 [==============================] - 0s 168us/sample - loss: 2.9075 - accuracy: 0.1886\n","Epoch 4/200\n","350/350 [==============================] - 0s 168us/sample - loss: 2.8609 - accuracy: 0.1886\n","Epoch 5/200\n","350/350 [==============================] - 0s 181us/sample - loss: 2.8392 - accuracy: 0.1886\n","Epoch 6/200\n","350/350 [==============================] - 0s 212us/sample - loss: 2.8387 - accuracy: 0.1886\n","Epoch 7/200\n","350/350 [==============================] - 0s 189us/sample - loss: 2.8186 - accuracy: 0.1886\n","Epoch 8/200\n","350/350 [==============================] - 0s 174us/sample - loss: 2.7816 - accuracy: 0.1886\n","Epoch 9/200\n","350/350 [==============================] - 0s 177us/sample - loss: 2.7390 - accuracy: 0.1886\n","Epoch 10/200\n","350/350 [==============================] - 0s 192us/sample - loss: 2.6911 - accuracy: 0.2086\n","Epoch 11/200\n","350/350 [==============================] - 0s 179us/sample - loss: 2.6433 - accuracy: 0.2229\n","Epoch 12/200\n","350/350 [==============================] - 0s 172us/sample - loss: 2.6064 - accuracy: 0.2057\n","Epoch 13/200\n","350/350 [==============================] - 0s 191us/sample - loss: 2.5313 - accuracy: 0.2171\n","Epoch 14/200\n","350/350 [==============================] - 0s 165us/sample - loss: 2.4694 - accuracy: 0.2400\n","Epoch 15/200\n","350/350 [==============================] - 0s 208us/sample - loss: 2.4229 - accuracy: 0.2286\n","Epoch 16/200\n","350/350 [==============================] - 0s 195us/sample - loss: 2.3583 - accuracy: 0.2686\n","Epoch 17/200\n","350/350 [==============================] - 0s 179us/sample - loss: 2.2921 - accuracy: 0.2686\n","Epoch 18/200\n","350/350 [==============================] - 0s 176us/sample - loss: 2.2836 - accuracy: 0.2371\n","Epoch 19/200\n","350/350 [==============================] - 0s 201us/sample - loss: 2.2143 - accuracy: 0.2629\n","Epoch 20/200\n","350/350 [==============================] - 0s 185us/sample - loss: 2.1236 - accuracy: 0.3200\n","Epoch 21/200\n","350/350 [==============================] - 0s 206us/sample - loss: 2.0690 - accuracy: 0.3314\n","Epoch 22/200\n","350/350 [==============================] - 0s 177us/sample - loss: 1.9474 - accuracy: 0.3457\n","Epoch 23/200\n","350/350 [==============================] - 0s 183us/sample - loss: 1.8330 - accuracy: 0.4000\n","Epoch 24/200\n","350/350 [==============================] - 0s 179us/sample - loss: 1.7384 - accuracy: 0.4543\n","Epoch 25/200\n","350/350 [==============================] - 0s 169us/sample - loss: 1.6579 - accuracy: 0.4971\n","Epoch 26/200\n","350/350 [==============================] - 0s 170us/sample - loss: 1.6058 - accuracy: 0.4829\n","Epoch 27/200\n","350/350 [==============================] - 0s 177us/sample - loss: 1.4584 - accuracy: 0.5314\n","Epoch 28/200\n","350/350 [==============================] - 0s 177us/sample - loss: 1.3833 - accuracy: 0.5857\n","Epoch 29/200\n","350/350 [==============================] - 0s 177us/sample - loss: 1.2722 - accuracy: 0.6657\n","Epoch 30/200\n","350/350 [==============================] - 0s 198us/sample - loss: 1.1608 - accuracy: 0.7000\n","Epoch 31/200\n","350/350 [==============================] - 0s 228us/sample - loss: 1.0949 - accuracy: 0.7457\n","Epoch 32/200\n","350/350 [==============================] - 0s 182us/sample - loss: 0.9996 - accuracy: 0.7800\n","Epoch 33/200\n","350/350 [==============================] - 0s 173us/sample - loss: 0.9281 - accuracy: 0.8343\n","Epoch 34/200\n","350/350 [==============================] - 0s 181us/sample - loss: 0.8446 - accuracy: 0.8600\n","Epoch 35/200\n","350/350 [==============================] - 0s 176us/sample - loss: 0.7814 - accuracy: 0.8914\n","Epoch 36/200\n","350/350 [==============================] - 0s 175us/sample - loss: 0.7297 - accuracy: 0.8886\n","Epoch 37/200\n","350/350 [==============================] - 0s 200us/sample - loss: 0.6575 - accuracy: 0.9114\n","Epoch 38/200\n","350/350 [==============================] - 0s 183us/sample - loss: 0.6301 - accuracy: 0.9171\n","Epoch 39/200\n","350/350 [==============================] - 0s 225us/sample - loss: 0.5927 - accuracy: 0.9371\n","Epoch 40/200\n","350/350 [==============================] - 0s 174us/sample - loss: 0.5421 - accuracy: 0.9486\n","Epoch 41/200\n","350/350 [==============================] - 0s 203us/sample - loss: 0.5067 - accuracy: 0.9571\n","Epoch 42/200\n","350/350 [==============================] - 0s 203us/sample - loss: 0.4750 - accuracy: 0.9600\n","Epoch 43/200\n","350/350 [==============================] - 0s 174us/sample - loss: 0.4354 - accuracy: 0.9600\n","Epoch 44/200\n","350/350 [==============================] - 0s 181us/sample - loss: 0.3816 - accuracy: 0.9886\n","Epoch 45/200\n","350/350 [==============================] - 0s 167us/sample - loss: 0.3549 - accuracy: 0.9771\n","Epoch 46/200\n","350/350 [==============================] - 0s 185us/sample - loss: 0.3341 - accuracy: 0.9771\n","Epoch 47/200\n","350/350 [==============================] - 0s 199us/sample - loss: 0.2985 - accuracy: 0.9886\n","Epoch 48/200\n","350/350 [==============================] - 0s 199us/sample - loss: 0.2818 - accuracy: 0.9914\n","Epoch 49/200\n","350/350 [==============================] - 0s 201us/sample - loss: 0.2623 - accuracy: 0.9857\n","Epoch 50/200\n","350/350 [==============================] - 0s 207us/sample - loss: 0.2457 - accuracy: 0.9857\n","Epoch 51/200\n","350/350 [==============================] - 0s 189us/sample - loss: 0.2289 - accuracy: 0.9971\n","Epoch 52/200\n","350/350 [==============================] - 0s 205us/sample - loss: 0.2239 - accuracy: 0.9886\n","Epoch 53/200\n","350/350 [==============================] - 0s 202us/sample - loss: 0.2092 - accuracy: 0.9943\n","Epoch 54/200\n","350/350 [==============================] - 0s 219us/sample - loss: 0.1978 - accuracy: 0.9914\n","Epoch 55/200\n","350/350 [==============================] - 0s 212us/sample - loss: 0.1872 - accuracy: 0.9943\n","Epoch 56/200\n","350/350 [==============================] - 0s 180us/sample - loss: 0.1809 - accuracy: 0.9914\n","Epoch 57/200\n","350/350 [==============================] - 0s 190us/sample - loss: 0.1903 - accuracy: 0.9943\n","Epoch 58/200\n","350/350 [==============================] - 0s 196us/sample - loss: 0.1683 - accuracy: 0.9971\n","Epoch 59/200\n","350/350 [==============================] - 0s 197us/sample - loss: 0.1559 - accuracy: 0.9943\n","Epoch 60/200\n","350/350 [==============================] - 0s 190us/sample - loss: 0.1475 - accuracy: 0.9943\n","Epoch 61/200\n","350/350 [==============================] - 0s 248us/sample - loss: 0.1380 - accuracy: 0.9943\n","Epoch 62/200\n","350/350 [==============================] - 0s 202us/sample - loss: 0.1359 - accuracy: 0.9914\n","Epoch 63/200\n","350/350 [==============================] - 0s 175us/sample - loss: 0.1301 - accuracy: 0.9943\n","Epoch 64/200\n","350/350 [==============================] - 0s 179us/sample - loss: 0.1231 - accuracy: 1.0000\n","Epoch 65/200\n","350/350 [==============================] - 0s 203us/sample - loss: 0.1197 - accuracy: 0.9943\n","Epoch 66/200\n","350/350 [==============================] - 0s 232us/sample - loss: 0.1245 - accuracy: 0.9914\n","Epoch 67/200\n","350/350 [==============================] - 0s 202us/sample - loss: 0.1356 - accuracy: 0.9886\n","Epoch 68/200\n","350/350 [==============================] - 0s 196us/sample - loss: 0.1673 - accuracy: 0.9829\n","Epoch 69/200\n","350/350 [==============================] - 0s 206us/sample - loss: 0.1574 - accuracy: 0.9829\n","Epoch 70/200\n","350/350 [==============================] - 0s 170us/sample - loss: 0.1410 - accuracy: 0.9914\n","Epoch 71/200\n","350/350 [==============================] - 0s 171us/sample - loss: 0.1207 - accuracy: 0.9943\n","Epoch 72/200\n","350/350 [==============================] - 0s 202us/sample - loss: 0.1019 - accuracy: 1.0000\n","Epoch 73/200\n","350/350 [==============================] - 0s 168us/sample - loss: 0.0910 - accuracy: 0.9914\n","Epoch 74/200\n","350/350 [==============================] - 0s 177us/sample - loss: 0.1006 - accuracy: 0.9914\n","Epoch 75/200\n","350/350 [==============================] - 0s 175us/sample - loss: 0.0881 - accuracy: 0.9886\n","Epoch 76/200\n","350/350 [==============================] - 0s 180us/sample - loss: 0.0793 - accuracy: 1.0000\n","Epoch 77/200\n","350/350 [==============================] - 0s 210us/sample - loss: 0.0714 - accuracy: 1.0000\n","Epoch 78/200\n","350/350 [==============================] - 0s 210us/sample - loss: 0.0662 - accuracy: 1.0000\n","Epoch 79/200\n","350/350 [==============================] - 0s 173us/sample - loss: 0.0627 - accuracy: 1.0000\n","Epoch 80/200\n","350/350 [==============================] - 0s 203us/sample - loss: 0.0610 - accuracy: 1.0000\n","Epoch 81/200\n","350/350 [==============================] - 0s 233us/sample - loss: 0.0660 - accuracy: 0.9971\n","Epoch 82/200\n","350/350 [==============================] - 0s 193us/sample - loss: 0.0639 - accuracy: 1.0000\n","Epoch 83/200\n","350/350 [==============================] - 0s 174us/sample - loss: 0.0620 - accuracy: 0.9943\n","Epoch 84/200\n","350/350 [==============================] - 0s 178us/sample - loss: 0.0646 - accuracy: 0.9886\n","Epoch 85/200\n","350/350 [==============================] - 0s 225us/sample - loss: 0.0607 - accuracy: 0.9971\n","Epoch 86/200\n","350/350 [==============================] - 0s 174us/sample - loss: 0.0609 - accuracy: 0.9971\n","Epoch 87/200\n","350/350 [==============================] - 0s 184us/sample - loss: 0.0549 - accuracy: 1.0000\n","Epoch 88/200\n","350/350 [==============================] - 0s 200us/sample - loss: 0.0496 - accuracy: 1.0000\n","Epoch 89/200\n","350/350 [==============================] - 0s 174us/sample - loss: 0.0478 - accuracy: 1.0000\n","Epoch 90/200\n","350/350 [==============================] - 0s 175us/sample - loss: 0.0466 - accuracy: 1.0000\n","Epoch 91/200\n","350/350 [==============================] - 0s 181us/sample - loss: 0.0457 - accuracy: 1.0000\n","Epoch 92/200\n","350/350 [==============================] - 0s 198us/sample - loss: 0.0425 - accuracy: 1.0000\n","Epoch 93/200\n","350/350 [==============================] - 0s 208us/sample - loss: 0.0425 - accuracy: 1.0000\n","Epoch 94/200\n","350/350 [==============================] - 0s 185us/sample - loss: 0.0406 - accuracy: 1.0000\n","Epoch 95/200\n","350/350 [==============================] - 0s 198us/sample - loss: 0.0403 - accuracy: 1.0000\n","Epoch 96/200\n","350/350 [==============================] - 0s 218us/sample - loss: 0.0413 - accuracy: 1.0000\n","Epoch 97/200\n","350/350 [==============================] - 0s 178us/sample - loss: 0.0397 - accuracy: 0.9971\n","Epoch 98/200\n","350/350 [==============================] - 0s 181us/sample - loss: 0.0426 - accuracy: 0.9971\n","Epoch 99/200\n","350/350 [==============================] - 0s 197us/sample - loss: 0.0515 - accuracy: 0.9914\n","Epoch 100/200\n","350/350 [==============================] - 0s 200us/sample - loss: 0.0428 - accuracy: 0.9971\n","Epoch 101/200\n","350/350 [==============================] - 0s 168us/sample - loss: 0.0409 - accuracy: 1.0000\n","Epoch 102/200\n","350/350 [==============================] - 0s 205us/sample - loss: 0.0374 - accuracy: 1.0000\n","Epoch 103/200\n","350/350 [==============================] - 0s 188us/sample - loss: 0.0425 - accuracy: 0.9943\n","Epoch 104/200\n","350/350 [==============================] - 0s 189us/sample - loss: 0.0474 - accuracy: 0.9943\n","Epoch 105/200\n","350/350 [==============================] - 0s 207us/sample - loss: 0.0418 - accuracy: 1.0000\n","Epoch 106/200\n","350/350 [==============================] - 0s 206us/sample - loss: 0.0372 - accuracy: 1.0000\n","Epoch 107/200\n","350/350 [==============================] - 0s 218us/sample - loss: 0.0330 - accuracy: 1.0000\n","Epoch 108/200\n","350/350 [==============================] - 0s 205us/sample - loss: 0.0310 - accuracy: 1.0000\n","Epoch 109/200\n","350/350 [==============================] - 0s 173us/sample - loss: 0.0290 - accuracy: 1.0000\n","Epoch 110/200\n","350/350 [==============================] - 0s 199us/sample - loss: 0.0273 - accuracy: 1.0000\n","Epoch 111/200\n","350/350 [==============================] - 0s 203us/sample - loss: 0.0265 - accuracy: 1.0000\n","Epoch 112/200\n","350/350 [==============================] - 0s 176us/sample - loss: 0.0261 - accuracy: 1.0000\n","Epoch 113/200\n","350/350 [==============================] - 0s 177us/sample - loss: 0.0255 - accuracy: 1.0000\n","Epoch 114/200\n","350/350 [==============================] - 0s 183us/sample - loss: 0.0253 - accuracy: 1.0000\n","Epoch 115/200\n","350/350 [==============================] - 0s 183us/sample - loss: 0.0242 - accuracy: 1.0000\n","Epoch 116/200\n","350/350 [==============================] - 0s 212us/sample - loss: 0.0235 - accuracy: 1.0000\n","Epoch 117/200\n","350/350 [==============================] - 0s 217us/sample - loss: 0.0232 - accuracy: 1.0000\n","Epoch 118/200\n","350/350 [==============================] - 0s 193us/sample - loss: 0.0228 - accuracy: 1.0000\n","Epoch 119/200\n","350/350 [==============================] - 0s 174us/sample - loss: 0.0229 - accuracy: 1.0000\n","Epoch 120/200\n","350/350 [==============================] - 0s 177us/sample - loss: 0.0234 - accuracy: 1.0000\n","Epoch 121/200\n","350/350 [==============================] - 0s 181us/sample - loss: 0.0217 - accuracy: 1.0000\n","Epoch 122/200\n","350/350 [==============================] - 0s 200us/sample - loss: 0.0217 - accuracy: 1.0000\n","Epoch 123/200\n","350/350 [==============================] - 0s 214us/sample - loss: 0.0216 - accuracy: 1.0000\n","Epoch 124/200\n","350/350 [==============================] - 0s 175us/sample - loss: 0.0205 - accuracy: 1.0000\n","Epoch 125/200\n","350/350 [==============================] - 0s 183us/sample - loss: 0.0203 - accuracy: 1.0000\n","Epoch 126/200\n","350/350 [==============================] - 0s 208us/sample - loss: 0.0196 - accuracy: 1.0000\n","Epoch 127/200\n","350/350 [==============================] - 0s 176us/sample - loss: 0.0191 - accuracy: 1.0000\n","Epoch 128/200\n","350/350 [==============================] - 0s 195us/sample - loss: 0.0187 - accuracy: 1.0000\n","Epoch 129/200\n","350/350 [==============================] - 0s 183us/sample - loss: 0.0183 - accuracy: 1.0000\n","Epoch 130/200\n","350/350 [==============================] - 0s 178us/sample - loss: 0.0184 - accuracy: 1.0000\n","Epoch 131/200\n","350/350 [==============================] - 0s 205us/sample - loss: 0.0187 - accuracy: 1.0000\n","Epoch 132/200\n","350/350 [==============================] - 0s 207us/sample - loss: 0.0282 - accuracy: 0.9971\n","Epoch 133/200\n","350/350 [==============================] - 0s 179us/sample - loss: 0.0206 - accuracy: 1.0000\n","Epoch 134/200\n","350/350 [==============================] - 0s 178us/sample - loss: 0.0308 - accuracy: 0.9943\n","Epoch 135/200\n","350/350 [==============================] - 0s 173us/sample - loss: 0.0344 - accuracy: 0.9971\n","Epoch 136/200\n","350/350 [==============================] - 0s 177us/sample - loss: 0.0291 - accuracy: 0.9971\n","Epoch 137/200\n","350/350 [==============================] - 0s 197us/sample - loss: 0.0400 - accuracy: 1.0000\n","Epoch 138/200\n","350/350 [==============================] - 0s 176us/sample - loss: 0.0294 - accuracy: 1.0000\n","Epoch 139/200\n","350/350 [==============================] - 0s 210us/sample - loss: 0.0224 - accuracy: 1.0000\n","Epoch 140/200\n","350/350 [==============================] - 0s 179us/sample - loss: 0.0191 - accuracy: 1.0000\n","Epoch 141/200\n","350/350 [==============================] - 0s 208us/sample - loss: 0.0173 - accuracy: 1.0000\n","Epoch 142/200\n","350/350 [==============================] - 0s 176us/sample - loss: 0.0164 - accuracy: 1.0000\n","Epoch 143/200\n","350/350 [==============================] - 0s 179us/sample - loss: 0.0157 - accuracy: 1.0000\n","Epoch 144/200\n","350/350 [==============================] - 0s 206us/sample - loss: 0.0152 - accuracy: 1.0000\n","Epoch 145/200\n","350/350 [==============================] - 0s 186us/sample - loss: 0.0148 - accuracy: 1.0000\n","Epoch 146/200\n","350/350 [==============================] - 0s 185us/sample - loss: 0.0144 - accuracy: 1.0000\n","Epoch 147/200\n","350/350 [==============================] - 0s 203us/sample - loss: 0.0141 - accuracy: 1.0000\n","Epoch 148/200\n","350/350 [==============================] - 0s 173us/sample - loss: 0.0139 - accuracy: 1.0000\n","Epoch 149/200\n","350/350 [==============================] - 0s 204us/sample - loss: 0.0136 - accuracy: 1.0000\n","Epoch 150/200\n","350/350 [==============================] - 0s 176us/sample - loss: 0.0131 - accuracy: 1.0000\n","Epoch 151/200\n","350/350 [==============================] - 0s 181us/sample - loss: 0.0130 - accuracy: 1.0000\n","Epoch 152/200\n","350/350 [==============================] - 0s 194us/sample - loss: 0.0128 - accuracy: 1.0000\n","Epoch 153/200\n","350/350 [==============================] - 0s 203us/sample - loss: 0.0125 - accuracy: 1.0000\n","Epoch 154/200\n","350/350 [==============================] - 0s 243us/sample - loss: 0.0123 - accuracy: 1.0000\n","Epoch 155/200\n","350/350 [==============================] - 0s 174us/sample - loss: 0.0121 - accuracy: 1.0000\n","Epoch 156/200\n","350/350 [==============================] - 0s 207us/sample - loss: 0.0121 - accuracy: 1.0000\n","Epoch 157/200\n","350/350 [==============================] - 0s 176us/sample - loss: 0.0116 - accuracy: 1.0000\n","Epoch 158/200\n","350/350 [==============================] - 0s 179us/sample - loss: 0.0115 - accuracy: 1.0000\n","Epoch 159/200\n","350/350 [==============================] - 0s 180us/sample - loss: 0.0113 - accuracy: 1.0000\n","Epoch 160/200\n","350/350 [==============================] - 0s 203us/sample - loss: 0.0112 - accuracy: 1.0000\n","Epoch 161/200\n","350/350 [==============================] - 0s 190us/sample - loss: 0.0110 - accuracy: 1.0000\n","Epoch 162/200\n","350/350 [==============================] - 0s 176us/sample - loss: 0.0109 - accuracy: 1.0000\n","Epoch 163/200\n","350/350 [==============================] - 0s 214us/sample - loss: 0.0108 - accuracy: 1.0000\n","Epoch 164/200\n","350/350 [==============================] - 0s 176us/sample - loss: 0.0107 - accuracy: 1.0000\n","Epoch 165/200\n","350/350 [==============================] - 0s 178us/sample - loss: 0.0103 - accuracy: 1.0000\n","Epoch 166/200\n","350/350 [==============================] - 0s 180us/sample - loss: 0.0102 - accuracy: 1.0000\n","Epoch 167/200\n","350/350 [==============================] - 0s 206us/sample - loss: 0.0100 - accuracy: 1.0000\n","Epoch 168/200\n","350/350 [==============================] - 0s 196us/sample - loss: 0.0099 - accuracy: 1.0000\n","Epoch 169/200\n","350/350 [==============================] - 0s 182us/sample - loss: 0.0098 - accuracy: 1.0000\n","Epoch 170/200\n","350/350 [==============================] - 0s 213us/sample - loss: 0.0096 - accuracy: 1.0000\n","Epoch 171/200\n","350/350 [==============================] - 0s 231us/sample - loss: 0.0097 - accuracy: 1.0000\n","Epoch 172/200\n","350/350 [==============================] - 0s 198us/sample - loss: 0.0094 - accuracy: 1.0000\n","Epoch 173/200\n","350/350 [==============================] - 0s 178us/sample - loss: 0.0093 - accuracy: 1.0000\n","Epoch 174/200\n","350/350 [==============================] - 0s 175us/sample - loss: 0.0092 - accuracy: 1.0000\n","Epoch 175/200\n","350/350 [==============================] - 0s 183us/sample - loss: 0.0090 - accuracy: 1.0000\n","Epoch 176/200\n","350/350 [==============================] - 0s 175us/sample - loss: 0.0091 - accuracy: 1.0000\n","Epoch 177/200\n","350/350 [==============================] - 0s 173us/sample - loss: 0.0090 - accuracy: 1.0000\n","Epoch 178/200\n","350/350 [==============================] - 0s 177us/sample - loss: 0.0088 - accuracy: 1.0000\n","Epoch 179/200\n","350/350 [==============================] - 0s 168us/sample - loss: 0.0086 - accuracy: 1.0000\n","Epoch 180/200\n","350/350 [==============================] - 0s 170us/sample - loss: 0.0085 - accuracy: 1.0000\n","Epoch 181/200\n","350/350 [==============================] - 0s 190us/sample - loss: 0.0084 - accuracy: 1.0000\n","Epoch 182/200\n","350/350 [==============================] - 0s 173us/sample - loss: 0.0083 - accuracy: 1.0000\n","Epoch 183/200\n","350/350 [==============================] - 0s 172us/sample - loss: 0.0082 - accuracy: 1.0000\n","Epoch 184/200\n","350/350 [==============================] - 0s 171us/sample - loss: 0.0080 - accuracy: 1.0000\n","Epoch 185/200\n","350/350 [==============================] - 0s 175us/sample - loss: 0.0080 - accuracy: 1.0000\n","Epoch 186/200\n","350/350 [==============================] - 0s 178us/sample - loss: 0.0078 - accuracy: 1.0000\n","Epoch 187/200\n","350/350 [==============================] - 0s 232us/sample - loss: 0.0078 - accuracy: 1.0000\n","Epoch 188/200\n","350/350 [==============================] - 0s 208us/sample - loss: 0.0077 - accuracy: 1.0000\n","Epoch 189/200\n","350/350 [==============================] - 0s 198us/sample - loss: 0.0076 - accuracy: 1.0000\n","Epoch 190/200\n","350/350 [==============================] - 0s 174us/sample - loss: 0.0075 - accuracy: 1.0000\n","Epoch 191/200\n","350/350 [==============================] - 0s 171us/sample - loss: 0.0074 - accuracy: 1.0000\n","Epoch 192/200\n","350/350 [==============================] - 0s 187us/sample - loss: 0.0073 - accuracy: 1.0000\n","Epoch 193/200\n","350/350 [==============================] - 0s 182us/sample - loss: 0.0072 - accuracy: 1.0000\n","Epoch 194/200\n","350/350 [==============================] - 0s 186us/sample - loss: 0.0071 - accuracy: 1.0000\n","Epoch 195/200\n","350/350 [==============================] - 0s 256us/sample - loss: 0.0071 - accuracy: 1.0000\n","Epoch 196/200\n","350/350 [==============================] - 0s 191us/sample - loss: 0.0070 - accuracy: 1.0000\n","Epoch 197/200\n","350/350 [==============================] - 0s 179us/sample - loss: 0.0069 - accuracy: 1.0000\n","Epoch 198/200\n","350/350 [==============================] - 0s 175us/sample - loss: 0.0068 - accuracy: 1.0000\n","Epoch 199/200\n","350/350 [==============================] - 0s 197us/sample - loss: 0.0067 - accuracy: 1.0000\n","Epoch 200/200\n","350/350 [==============================] - 0s 204us/sample - loss: 0.0067 - accuracy: 1.0000\n","CPU times: user 18.9 s, sys: 1.71 s, total: 20.6 s\n","Wall time: 17.2 s\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fc0495a3d30>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"v7Bf90ATg98K","colab_type":"text"},"source":["## **Generate Text:** Analysis"]},{"cell_type":"markdown","metadata":{"id":"6uBnrJJe0fgz","colab_type":"text"},"source":["### Define generate_seq function"]},{"cell_type":"code","metadata":{"id":"OFq9I4aCDI6k","colab_type":"code","colab":{}},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# generate a sequence of characters with a language model\n","def generate_seq(model, char_dic, seq_length, seed_text, n_chars):\n","\tin_text = seed_text\t\t\t\t\t\t\t\t\t# seed text + generated text\n","\t# generate a fixed number of characters\n","\tfor i in range(n_chars):\n","\t\t# encode the characters as integers\n","\t\tencoded = [char_dic[char] for char in in_text]\n","\t\t# truncate sequences to a fixed length\n","\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #  원하는 사이즈만큼 가져와서 그것만 남겨둔다. \n","\t\tencoded = tf.keras.utils.to_categorical(encoded, num_classes=len(char_dic))\n","\t\t#encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n","\t\t# predict character\n","\t\tyhat = model.predict_classes(encoded, verbose=0)\n","\t\t# reverse map integer to character\n","\t\tfor char, index in char_dic.items():\n","\t\t\tif index == yhat:\n","\t\t\t\tbreak\n","\t\t# append to input\n","\t\tin_text += char\n","\treturn in_text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2_tIRa7h1PUA"},"source":["### Generate a sequence of characters"]},{"cell_type":"markdown","metadata":{"id":"hhgIpBwG06uc","colab_type":"text"},"source":["학습문장: if you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea."]},{"cell_type":"code","metadata":{"id":"xC3D8JPW1O0Y","colab_type":"code","outputId":"9f30f465-dabf-4541-92b8-96907471c3e5","executionInfo":{"status":"ok","timestamp":1575973098390,"user_tz":-540,"elapsed":14692,"user":{"displayName":"석민창","photoUrl":"","userId":"14498853270593904544"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["# test start of rhyme\n","print('Trigger characters: \"want to bu \"\\nResult: \\n\"{}\"'.format( \n","      generate_seq(model, char_dic, 10, 'want to bu', 200)))\n","# test mid-line\n","print('\\nTrigger characters: \"collect wo\"\\nResult: \\n\"{}\"'.format(\n","      generate_seq(model, char_dic, 10, 'collect wo', 100)))\n","# test not in original\n","print('\\nTrigger characters: \"rather tea\"\\nResult: \"{}\"'.format(\n","      generate_seq(model, char_dic, 10, 'rather than', 200)))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Trigger characters: \"want to bu \"\n","Result: \n","\"want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.if you want to build a ship, don't dr\"\n","\n","Trigger characters: \"collect wo\"\n","Result: \n","\"collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of \"\n","\n","Trigger characters: \"rather tea\"\n","Result: \"rather thanh lee    i tsiu    t aepp d   eoeodut  tynnubl  r  bbl eeeeusssst   mentnaa   iibh oo     ndddadpuussst,ooopetttttn hs     k 'nddpduuum tttobllll    r rrppdeeaaaccruntmooohtpl lggnlleehhhhhisoy gggyyt\"\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GnaUZbi_hIHw","colab_type":"text"},"source":["## **실습과제**"]},{"cell_type":"markdown","metadata":{"id":"07YM3eLIWxSY","colab_type":"text"},"source":["### 과제 1. 더 긴문장을 생성하도록 해보자\n","####-- 학습문장이 \" ...  immensity of the sea.\"로 끝났다. 첫번째 생성문장이 뒷부분이 이상하다. 180자로 학습한 한계 때문일까? \n","#### -- 학습문장 전체를 한번더 반복하여 학습문장을 360자로 확장하고 결과를 비교해 보자\n","\n","\n","\n","#### -- 결과를 분석하자: \n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"75OB3kuBXZjg","colab_type":"text"},"source":["1번의 경우 180자 이후부터 다시 처음부터 문장이 시작되는 것을 확인할 수 있다.\n","\n","결과적으로 기존의 가장 끝 단어가 나타난 이후 모델이 학습한 것은 다시 처음부터 이어지는 문장이다. 그렇기 때문에 내부에서 LSTM 을 통해서 학습하는 과정에서 . 이후의 다음 문장은 복사 붙여넣기 한 첫 if가 나오는 것이라고 학습하고, 이를 학습한대로 뱉어낸 것이라 생각된다."]},{"cell_type":"markdown","metadata":{"id":"5uqymQOfGZW2","colab_type":"text"},"source":["### 과제 2. 세번째 생성문장은 여전히 문제가 많다. 모델을 개선해 보자 \n","####-- LSTM layer를 추가해 보자 \n","####-- 더 무엇을 해볼까? 생각해 보자\n","####-- 결과를 분석하자:"]},{"cell_type":"markdown","metadata":{"id":"2QvKz0RVNwH1","colab_type":"text"},"source":["LSTM layer를 추가한다고 좋은 결과가 나오는 것은 아니라는 사실을 위에 새롭게 layer를 더해서 수정한 코드를 보면 알 수 있다.\n","\n","이유는 지금의 코드는 앞전의 글자들이 나올 때 마지막 글자가 어떤 글자가 나오는 것이 좋은지 그 확률을 계산하고 있다.\n","\n","즉 전체 시퀀스, 각 글자간 어떻게 연결되고 있는지를 확인하고 있는 것이 아니기 때문에 문장이 구성되는 것을 신경쓰지 않고 아무 글자나 해당 위치에 나오기 가장 적합한 글자를 뱉어내고 있는 것이다.\n","\n","이를 해결하기 위해서는 시퀀스의 셀들이 모두 결과를 꺼내도록 해야한다. 즉, return_sequences를 True로 바꿔야 하는 것이다."]},{"cell_type":"code","metadata":{"id":"i7rl_xHONzM_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}